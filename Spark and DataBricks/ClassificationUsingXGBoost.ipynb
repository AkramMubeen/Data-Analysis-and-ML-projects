{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25b5d7a9-0ca5-4a93-a7e9-329d2eb1ac23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We would need latest sklearn version to use 'get_feature_names_out' of Columntransformer.So upgrading scikit-learn first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "285c37ac-9a87-44be-bbd8-2cef2cff411a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'1.1.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a7a9bc-9979-4f94-b404-2b078e393160",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting xgboost\n  Downloading xgboost-2.0.0-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 297.1/297.1 MB 1.9 MB/s eta 0:00:00\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.10/site-packages (from xgboost) (1.21.5)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.10/site-packages (from xgboost) (1.9.1)\nInstalling collected packages: xgboost\nSuccessfully installed xgboost-2.0.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f423eb89-dff6-4ea0-8473-c2faa61e6e75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31ec923c-b572-4edf-9cc0-4d5f4c8cb31b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: '1.1.3'"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "def33889-163f-4278-b519-4acecea2aba3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import xgboost as xgb\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a239893-a4c3-4d48-b52a-d05ac38f9183",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Loading the already splitted train and test dataframes from demo-02.\n",
    "Checking the number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9455a224-0fea-473f-8ebd-234ef9307aa3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((34062, 10), (14598, 10), (34062, 1), (14598, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.read_csv('/dbfs/FileStore/tables/processed_x_train.csv')\n",
    "X_test = pd.read_csv('/dbfs/FileStore/tables/processed_x_test.csv')\n",
    "y_train = pd.read_csv('/dbfs/FileStore/tables/processed_y_train.csv')\n",
    "y_test = pd.read_csv('/dbfs/FileStore/tables/processed_y_test.csv')\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45b006eb-b0d2-4e48-a78a-8cb661985842",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>department</th>\n",
       "      <th>education</th>\n",
       "      <th>gender</th>\n",
       "      <th>recruitment_channel</th>\n",
       "      <th>no_of_trainings</th>\n",
       "      <th>age</th>\n",
       "      <th>previous_year_rating</th>\n",
       "      <th>length_of_service</th>\n",
       "      <th>awards_won?</th>\n",
       "      <th>avg_training_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Technology</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>f</td>\n",
       "      <td>other</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Procurement</td>\n",
       "      <td>Master's &amp; above</td>\n",
       "      <td>f</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Technology</td>\n",
       "      <td>Master's &amp; above</td>\n",
       "      <td>f</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sales &amp; Marketing</td>\n",
       "      <td>Bachelor's</td>\n",
       "      <td>m</td>\n",
       "      <td>sourcing</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Technology</td>\n",
       "      <td>Master's &amp; above</td>\n",
       "      <td>f</td>\n",
       "      <td>other</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          department         education  ... awards_won? avg_training_score\n",
       "0         Technology        Bachelor's  ...           0                 81\n",
       "1        Procurement  Master's & above  ...           0                 65\n",
       "2         Technology  Master's & above  ...           0                 81\n",
       "3  Sales & Marketing        Bachelor's  ...           0                 45\n",
       "4         Technology  Master's & above  ...           0                 77\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ce8f444-11eb-4cb9-ae0e-c7d91491598e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Categorical features are listed and One hot encoder is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e10fb21d-129c-4749-938c-fd74c59d6b0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = ['department', 'recruitment_channel', 'education', 'gender']\n",
    "\n",
    "categorical_transformer = OneHotEncoder(drop = 'first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a1cb247-b5c7-4f82-bd93-c0b49846a521",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "ColumnTransformer : This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space. Here We are using Column Transformer to one hot encode categorical features defined above and Standard scale remaining numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7359b342-63bb-4d21-bfa2-e0fc3d5453ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColumnTransformer(remainder=StandardScaler(),\n                  transformers=[('cat', OneHotEncoder(drop='first'),\n                                 ['department', 'recruitment_channel',\n                                  'education', 'gender'])])\n"
     ]
    }
   ],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [('cat', categorical_transformer, categorical_features)], \n",
    "    remainder = StandardScaler() \n",
    ")\n",
    "\n",
    "print(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03bbdfd3-b533-48ce-a453-63ce91258aca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We are defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f1740fc-043c-4722-b5e0-c827d1fabf80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77a76ca7-2050-4e98-aea3-44c49b60ccf7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Pipeline is defined with preprocessing and model building steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0784b32-f040-4249-9a76-f52fb4321b23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    steps = [('preprocessor', preprocessor), ('classifier',  rf_model)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e34c042-5084-4e71-ba58-4387d5813a34",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This task seems well suited to a random forest classifier, since the output is binary and there may be interactions between multiple variables.\n",
    "\n",
    "The following code builds a simple classifier using scikit-learn. It uses MLflow to keep track of the model accuracy, and to save the model for later use.We are logging the metrics using log_metric method.Also infer signature is defined which is required to validate the inputs. Model is also logged here. Metrics observed after  run is auc: 0.765\n",
    "test_accuracy: 0.933\n",
    "test_f1_score: 0.468\n",
    "test_precision_score: 0.757\n",
    "test_recall_score: 0.339\n",
    "\n",
    "Note that we are not autologging parameters here.Note that signatures of training data can also be seen by clicking MLModel under rfmodel under  Model artifacts as we are using infer_signature.\n",
    "\n",
    "- log_models=False is configured in autolog. This prevents MLflow to automatically log the model, as it is done manually later.\n",
    "- infer_signature is a convenient method to try to infer the signature directly from inputs and outpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82f022d7-3437-4491-a6eb-2f43128d5fb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/10/15 13:08:40 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n2023/10/15 13:08:42 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n2023/10/15 13:08:42 INFO mlflow.tracking.fluent: Autologging successfully enabled for xgboost.\n2023/10/15 13:08:42 INFO mlflow._spark_autologging: Autologging successfully enabled for spark.\n2023/10/15 13:08:42 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n2023/10/15 13:08:42 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n2023/10/15 13:08:44 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-af8147ed-d940-457c-bb64-6624008909b1/lib/python3.10/site-packages/mlflow/data/pandas_dataset.py:116: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n2023/10/15 13:08:55 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-af8147ed-d940-457c-bb64-6624008909b1/lib/python3.10/site-packages/mlflow/data/pandas_dataset.py:116: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n2023/10/15 13:08:56 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-af8147ed-d940-457c-bb64-6624008909b1/lib/python3.10/site-packages/mlflow/data/pandas_dataset.py:116: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n2023/10/15 13:08:58 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-af8147ed-d940-457c-bb64-6624008909b1/lib/python3.10/site-packages/mlflow/data/pandas_dataset.py:116: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-af8147ed-d940-457c-bb64-6624008909b1/lib/python3.10/site-packages/mlflow/models/signature.py:212: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n  inputs = _infer_schema(model_input) if model_input is not None else None\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "mlflow.autolog(log_models=False)\n",
    "\n",
    "with mlflow.start_run() as run1:\n",
    "    y_train = y_train.values.ravel()\n",
    "    y_test = y_test.values.ravel()\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    predictions =  pipeline.predict(X_test)\n",
    "    predictions_proba = pipeline.predict_proba(X_test)\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, predictions)\n",
    "    test_precision_score = precision_score(y_test, predictions)\n",
    "    test_recall_score = recall_score(y_test, predictions)\n",
    "    test_f1_score = f1_score(y_test, predictions)\n",
    "    auc_score = roc_auc_score(y_test,  predictions_proba[:, 1])\n",
    "    \n",
    "    mlflow.log_metric('test_accuracy',  test_accuracy)\n",
    "    mlflow.log_metric('test_precision', test_precision_score)\n",
    "    mlflow.log_metric('test_recall', test_recall_score)\n",
    "    mlflow.log_metric('test_f1', test_f1_score)\n",
    "    mlflow.log_metric('auc_score', auc_score)\n",
    "    \n",
    "    cf_matrix = confusion_matrix(y_test, predictions)\n",
    "    \n",
    "    sns.heatmap(cf_matrix, annot=True)\n",
    "    plt.savefig(\"confusion_matrix_test.png\")\n",
    "    mlflow.log_artifact(\"confusion_matrix_test.png\") \n",
    "    \n",
    "    signature = infer_signature(X_train, pipeline.predict(X_train))\n",
    "    mlflow.sklearn.log_model(sk_model = pipeline, artifact_path = 'rf_model', signature = signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6739bb05-10fd-43e0-a99e-d194325ac660",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We would be using XGBOOST to train our data.So data is preprocessed in order to feed into the model.Preprocessed sample can be viewed. Note that scikit learn api is not supported for mlflow autologging.So scikit learn pipeline is not used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8c8a3bb-96d7-441d-90b3-c0b13449e4b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat__department_Finance</th>\n",
       "      <th>cat__department_HR</th>\n",
       "      <th>cat__department_Legal</th>\n",
       "      <th>cat__department_Operations</th>\n",
       "      <th>cat__department_Procurement</th>\n",
       "      <th>cat__department_R&amp;D</th>\n",
       "      <th>cat__department_Sales &amp; Marketing</th>\n",
       "      <th>cat__department_Technology</th>\n",
       "      <th>cat__recruitment_channel_referred</th>\n",
       "      <th>cat__recruitment_channel_sourcing</th>\n",
       "      <th>cat__education_Below Secondary</th>\n",
       "      <th>cat__education_Master's &amp; above</th>\n",
       "      <th>cat__gender_m</th>\n",
       "      <th>cat__gender_others</th>\n",
       "      <th>remainder__no_of_trainings</th>\n",
       "      <th>remainder__age</th>\n",
       "      <th>remainder__previous_year_rating</th>\n",
       "      <th>remainder__length_of_service</th>\n",
       "      <th>remainder__awards_won?</th>\n",
       "      <th>remainder__avg_training_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5971</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.418813</td>\n",
       "      <td>-0.081015</td>\n",
       "      <td>1.317903</td>\n",
       "      <td>0.630703</td>\n",
       "      <td>-0.153289</td>\n",
       "      <td>-1.100365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32537</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.418813</td>\n",
       "      <td>-1.007128</td>\n",
       "      <td>-0.270070</td>\n",
       "      <td>-0.314956</td>\n",
       "      <td>-0.153289</td>\n",
       "      <td>1.765156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17366</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.244234</td>\n",
       "      <td>-1.139429</td>\n",
       "      <td>-1.064057</td>\n",
       "      <td>-1.024200</td>\n",
       "      <td>-0.153289</td>\n",
       "      <td>1.689748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21097</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.418813</td>\n",
       "      <td>0.183588</td>\n",
       "      <td>-0.270070</td>\n",
       "      <td>0.394288</td>\n",
       "      <td>-0.153289</td>\n",
       "      <td>-1.251182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29486</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.418813</td>\n",
       "      <td>-0.610222</td>\n",
       "      <td>-1.064057</td>\n",
       "      <td>-0.314956</td>\n",
       "      <td>-0.153289</td>\n",
       "      <td>-0.949548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18719</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.244234</td>\n",
       "      <td>-0.081015</td>\n",
       "      <td>-1.858044</td>\n",
       "      <td>-0.314956</td>\n",
       "      <td>-0.153289</td>\n",
       "      <td>-1.024957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.418813</td>\n",
       "      <td>0.051286</td>\n",
       "      <td>0.523917</td>\n",
       "      <td>0.157874</td>\n",
       "      <td>-0.153289</td>\n",
       "      <td>0.935663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19156</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.418813</td>\n",
       "      <td>0.051286</td>\n",
       "      <td>-0.270070</td>\n",
       "      <td>0.630703</td>\n",
       "      <td>-0.153289</td>\n",
       "      <td>-0.798732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4951</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.244234</td>\n",
       "      <td>0.051286</td>\n",
       "      <td>0.523917</td>\n",
       "      <td>0.394288</td>\n",
       "      <td>-0.153289</td>\n",
       "      <td>-0.120055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8789</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.418813</td>\n",
       "      <td>0.977398</td>\n",
       "      <td>0.523917</td>\n",
       "      <td>0.157874</td>\n",
       "      <td>-0.153289</td>\n",
       "      <td>0.784846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cat__department_Finance  ...  remainder__avg_training_score\n",
       "5971                       0.0  ...                      -1.100365\n",
       "32537                      0.0  ...                       1.765156\n",
       "17366                      0.0  ...                       1.689748\n",
       "21097                      0.0  ...                      -1.251182\n",
       "29486                      0.0  ...                      -0.949548\n",
       "18719                      0.0  ...                      -1.024957\n",
       "1926                       0.0  ...                       0.935663\n",
       "19156                      0.0  ...                      -0.798732\n",
       "4951                       0.0  ...                      -0.120055\n",
       "8789                       0.0  ...                       0.784846\n",
       "\n",
       "[10 rows x 20 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_processed = pd.DataFrame(preprocessor.fit_transform(X_train),\n",
    "                                 columns =  preprocessor.get_feature_names_out(),\n",
    "                                 index = X_train.index)\n",
    "\n",
    "X_test_processed = pd.DataFrame(preprocessor.transform(X_test),\n",
    "                                columns = preprocessor.get_feature_names_out(), \n",
    "                                index = X_test.index)\n",
    "X_train_processed.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62c9c809-0233-44e7-8415-2eb6e04a39ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Parameters for training are defined.Note that first we will run the below cell by uncommenting. In this case XGBOOST model is increasing precision better than recall.As this dataset is highly imbalanced,We would use 'scale_pos_weight' to treat the imbalance. Negative/Positive ratio is around 11.After increasing 'scale_pos_weight' it can be seen that recall increased significantly but precision reduced significantly.\n",
    "ref.link for 'scale_pos_weight' parameter-\n",
    "\n",
    "Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddaa97a7-7feb-403a-b8e7-645c71ba5e88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# param = {'max_depth': 3, 'eta': 0.1, 'min_child_weight': 3, 'objective': 'binary:logistic', 'eval_metric':'logloss'}\n",
    "# param['nthread'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba742b8-cdfc-4e5e-b7c7-41baa9860fb0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "param = {'max_depth': 3, 'eta': 0.1, 'min_child_weight': 3, 'objective': 'binary:logistic', 'scale_pos_weight': 11, 'eval_metric':'logloss'}\n",
    "param['nthread'] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f24fddc3-9bf4-41fe-a36c-3d30e2a73c93",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "With  autologging, hyperparameters and the trained model are automatically logged to MLflow.Note that predictions from booster model is probabilities.So we are round it to get the prediction label.But in case of computation of roc_auc score value we need the probabilities.\n",
    " Metrics oberved after training are \n",
    " auc: 0.811\n",
    "test_accuracy: 0.942\n",
    "test_f1_score: 0.511\n",
    "test_precision_score: 0.933\n",
    "test_recall_score: 0.352\n",
    "test-logloss: 0.202\n",
    "So using XGBOOST , AUC,F1 score,precision have improved.We can use other Data preprocessing steps like oversampling and hyperparameter tuning further to improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3691c1e7-c805-4305-bc3b-3ab4e870398a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-logloss:0.69600\n[1]\ttest-logloss:0.68163\n[2]\ttest-logloss:0.67045\n[3]\ttest-logloss:0.65927\n[4]\ttest-logloss:0.64994\n[5]\ttest-logloss:0.64189\n[6]\ttest-logloss:0.63378\n[7]\ttest-logloss:0.62634\n[8]\ttest-logloss:0.62097\n[9]\ttest-logloss:0.61572\n[10]\ttest-logloss:0.61122\n[11]\ttest-logloss:0.60678\n[12]\ttest-logloss:0.60229\n[13]\ttest-logloss:0.59681\n[14]\ttest-logloss:0.59406\n[15]\ttest-logloss:0.59110\n[16]\ttest-logloss:0.58821\n[17]\ttest-logloss:0.58590\n[18]\ttest-logloss:0.58379\n[19]\ttest-logloss:0.57939\n[20]\ttest-logloss:0.57764\n[21]\ttest-logloss:0.57398\n[22]\ttest-logloss:0.57251\n[23]\ttest-logloss:0.57106\n[24]\ttest-logloss:0.56492\n[25]\ttest-logloss:0.56404\n[26]\ttest-logloss:0.56307\n[27]\ttest-logloss:0.56195\n[28]\ttest-logloss:0.55913\n[29]\ttest-logloss:0.55421\n[30]\ttest-logloss:0.55351\n[31]\ttest-logloss:0.55275\n[32]\ttest-logloss:0.55218\n[33]\ttest-logloss:0.55149\n[34]\ttest-logloss:0.55101\n[35]\ttest-logloss:0.54854\n[36]\ttest-logloss:0.54434\n[37]\ttest-logloss:0.54389\n[38]\ttest-logloss:0.54148\n[39]\ttest-logloss:0.54116\n[40]\ttest-logloss:0.54076\n[41]\ttest-logloss:0.54043\n[42]\ttest-logloss:0.53962\n[43]\ttest-logloss:0.53919\n[44]\ttest-logloss:0.53877\n[45]\ttest-logloss:0.53647\n[46]\ttest-logloss:0.53489\n[47]\ttest-logloss:0.53455\n[48]\ttest-logloss:0.53439\n[49]\ttest-logloss:0.53412\n[50]\ttest-logloss:0.53381\n[51]\ttest-logloss:0.53029\n[52]\ttest-logloss:0.52989\n[53]\ttest-logloss:0.52975\n[54]\ttest-logloss:0.52928\n[55]\ttest-logloss:0.52899\n[56]\ttest-logloss:0.52877\n[57]\ttest-logloss:0.52737\n[58]\ttest-logloss:0.52721\n[59]\ttest-logloss:0.52418\n[60]\ttest-logloss:0.52242\n[61]\ttest-logloss:0.52147\n[62]\ttest-logloss:0.52034\n[63]\ttest-logloss:0.52031\n[64]\ttest-logloss:0.51920\n[65]\ttest-logloss:0.51910\n[66]\ttest-logloss:0.51784\n[67]\ttest-logloss:0.51636\n[68]\ttest-logloss:0.51618\n[69]\ttest-logloss:0.51610\n[70]\ttest-logloss:0.51470\n[71]\ttest-logloss:0.51439\n[72]\ttest-logloss:0.51433\n[73]\ttest-logloss:0.51415\n[74]\ttest-logloss:0.51331\n[75]\ttest-logloss:0.51184\n[76]\ttest-logloss:0.51180\n[77]\ttest-logloss:0.51173\n[78]\ttest-logloss:0.51161\n[79]\ttest-logloss:0.51098\n[80]\ttest-logloss:0.50965\n[81]\ttest-logloss:0.50952\n[82]\ttest-logloss:0.50944\n[83]\ttest-logloss:0.50834\n[84]\ttest-logloss:0.50742\n[85]\ttest-logloss:0.50638\n[86]\ttest-logloss:0.50577\n[87]\ttest-logloss:0.50561\n[88]\ttest-logloss:0.50547\n[89]\ttest-logloss:0.50455\n[90]\ttest-logloss:0.50452\n[91]\ttest-logloss:0.50453\n[92]\ttest-logloss:0.50441\n[93]\ttest-logloss:0.50443\n[94]\ttest-logloss:0.50383\n[95]\ttest-logloss:0.50376\n[96]\ttest-logloss:0.50360\n[97]\ttest-logloss:0.50274\n[98]\ttest-logloss:0.50259\n[99]\ttest-logloss:0.50254\n[100]\ttest-logloss:0.50249\n[101]\ttest-logloss:0.50245\n[102]\ttest-logloss:0.50191\n[103]\ttest-logloss:0.50084\n[104]\ttest-logloss:0.50075\n[105]\ttest-logloss:0.50076\n[106]\ttest-logloss:0.50068\n[107]\ttest-logloss:0.50053\n[108]\ttest-logloss:0.50016\n[109]\ttest-logloss:0.49945\n[110]\ttest-logloss:0.49877\n[111]\ttest-logloss:0.49874\n[112]\ttest-logloss:0.49856\n[113]\ttest-logloss:0.49808\n[114]\ttest-logloss:0.49797\n[115]\ttest-logloss:0.49798\n[116]\ttest-logloss:0.49795\n[117]\ttest-logloss:0.49735\n[118]\ttest-logloss:0.49633\n[119]\ttest-logloss:0.49590\n[120]\ttest-logloss:0.49532\n[121]\ttest-logloss:0.49495\n[122]\ttest-logloss:0.49495\n[123]\ttest-logloss:0.49452\n[124]\ttest-logloss:0.49448\n[125]\ttest-logloss:0.49408\n[126]\ttest-logloss:0.49368\n[127]\ttest-logloss:0.49368\n[128]\ttest-logloss:0.49366\n[129]\ttest-logloss:0.49334\n[130]\ttest-logloss:0.49240\n[131]\ttest-logloss:0.49232\n[132]\ttest-logloss:0.49222\n[133]\ttest-logloss:0.49216\n[134]\ttest-logloss:0.49205\n[135]\ttest-logloss:0.49192\n[136]\ttest-logloss:0.49190\n[137]\ttest-logloss:0.49159\n[138]\ttest-logloss:0.49158\n[139]\ttest-logloss:0.49147\n[140]\ttest-logloss:0.49144\n[141]\ttest-logloss:0.49137\n[142]\ttest-logloss:0.49105\n[143]\ttest-logloss:0.49105\n[144]\ttest-logloss:0.49086\n[145]\ttest-logloss:0.49041\n[146]\ttest-logloss:0.49010\n[147]\ttest-logloss:0.49006\n[148]\ttest-logloss:0.48984\n[149]\ttest-logloss:0.48947\n[150]\ttest-logloss:0.48935\n[151]\ttest-logloss:0.48924\n[152]\ttest-logloss:0.48906\n[153]\ttest-logloss:0.48876\n[154]\ttest-logloss:0.48871\n[155]\ttest-logloss:0.48867\n[156]\ttest-logloss:0.48842\n[157]\ttest-logloss:0.48841\n[158]\ttest-logloss:0.48819\n[159]\ttest-logloss:0.48737\n[160]\ttest-logloss:0.48688\n[161]\ttest-logloss:0.48684\n[162]\ttest-logloss:0.48684\n[163]\ttest-logloss:0.48683\n[164]\ttest-logloss:0.48675\n[165]\ttest-logloss:0.48671\n[166]\ttest-logloss:0.48666\n[167]\ttest-logloss:0.48661\n[168]\ttest-logloss:0.48659\n[169]\ttest-logloss:0.48624\n[170]\ttest-logloss:0.48611\n[171]\ttest-logloss:0.48606\n[172]\ttest-logloss:0.48606\n[173]\ttest-logloss:0.48600\n[174]\ttest-logloss:0.48576\n[175]\ttest-logloss:0.48557\n[176]\ttest-logloss:0.48554\n[177]\ttest-logloss:0.48523\n[178]\ttest-logloss:0.48490\n[179]\ttest-logloss:0.48486\n[180]\ttest-logloss:0.48451\n[181]\ttest-logloss:0.48430\n[182]\ttest-logloss:0.48399\n[183]\ttest-logloss:0.48396\n[184]\ttest-logloss:0.48336\n[185]\ttest-logloss:0.48333\n[186]\ttest-logloss:0.48317\n[187]\ttest-logloss:0.48312\n[188]\ttest-logloss:0.48310\n[189]\ttest-logloss:0.48310\n[190]\ttest-logloss:0.48311\n[191]\ttest-logloss:0.48307\n[192]\ttest-logloss:0.48305\n[193]\ttest-logloss:0.48302\n[194]\ttest-logloss:0.48289\n[195]\ttest-logloss:0.48258\n[196]\ttest-logloss:0.48260\n[197]\ttest-logloss:0.48251\n[198]\ttest-logloss:0.48247\n[199]\ttest-logloss:0.48222\n[200]\ttest-logloss:0.48134\n[201]\ttest-logloss:0.48132\n[202]\ttest-logloss:0.48126\n[203]\ttest-logloss:0.48121\n[204]\ttest-logloss:0.48044\n[205]\ttest-logloss:0.48038\n[206]\ttest-logloss:0.48016\n[207]\ttest-logloss:0.47989\n[208]\ttest-logloss:0.47932\n[209]\ttest-logloss:0.47916\n[210]\ttest-logloss:0.47914\n[211]\ttest-logloss:0.47910\n[212]\ttest-logloss:0.47905\n[213]\ttest-logloss:0.47881\n[214]\ttest-logloss:0.47876\n[215]\ttest-logloss:0.47848\n[216]\ttest-logloss:0.47847\n[217]\ttest-logloss:0.47838\n[218]\ttest-logloss:0.47819\n[219]\ttest-logloss:0.47813\n[220]\ttest-logloss:0.47814\n[221]\ttest-logloss:0.47807\n[222]\ttest-logloss:0.47790\n[223]\ttest-logloss:0.47745\n[224]\ttest-logloss:0.47726\n[225]\ttest-logloss:0.47729\n[226]\ttest-logloss:0.47727\n[227]\ttest-logloss:0.47714\n[228]\ttest-logloss:0.47692\n[229]\ttest-logloss:0.47695\n[230]\ttest-logloss:0.47696\n[231]\ttest-logloss:0.47696\n[232]\ttest-logloss:0.47690\n[233]\ttest-logloss:0.47690\n[234]\ttest-logloss:0.47686\n[235]\ttest-logloss:0.47685\n[236]\ttest-logloss:0.47684\n[237]\ttest-logloss:0.47669\n[238]\ttest-logloss:0.47667\n[239]\ttest-logloss:0.47661\n[240]\ttest-logloss:0.47658\n[241]\ttest-logloss:0.47658\n[242]\ttest-logloss:0.47658\n[243]\ttest-logloss:0.47640\n[244]\ttest-logloss:0.47627\n[245]\ttest-logloss:0.47615\n[246]\ttest-logloss:0.47575\n[247]\ttest-logloss:0.47574\n[248]\ttest-logloss:0.47561\n[249]\ttest-logloss:0.47530\n[250]\ttest-logloss:0.47517\n[251]\ttest-logloss:0.47507\n[252]\ttest-logloss:0.47507\n[253]\ttest-logloss:0.47507\n[254]\ttest-logloss:0.47473\n[255]\ttest-logloss:0.47470\n[256]\ttest-logloss:0.47443\n[257]\ttest-logloss:0.47437\n[258]\ttest-logloss:0.47431\n[259]\ttest-logloss:0.47398\n[260]\ttest-logloss:0.47393\n[261]\ttest-logloss:0.47373\n[262]\ttest-logloss:0.47344\n[263]\ttest-logloss:0.47340\n[264]\ttest-logloss:0.47339\n[265]\ttest-logloss:0.47315\n[266]\ttest-logloss:0.47289\n[267]\ttest-logloss:0.47268\n[268]\ttest-logloss:0.47263\n[269]\ttest-logloss:0.47263\n[270]\ttest-logloss:0.47263\n[271]\ttest-logloss:0.47258\n[272]\ttest-logloss:0.47255\n[273]\ttest-logloss:0.47244\n[274]\ttest-logloss:0.47242\n[275]\ttest-logloss:0.47239\n[276]\ttest-logloss:0.47239\n[277]\ttest-logloss:0.47239\n[278]\ttest-logloss:0.47238\n[279]\ttest-logloss:0.47233\n[280]\ttest-logloss:0.47211\n[281]\ttest-logloss:0.47210\n[282]\ttest-logloss:0.47208\n[283]\ttest-logloss:0.47207\n[284]\ttest-logloss:0.47200\n[285]\ttest-logloss:0.47190\n[286]\ttest-logloss:0.47188\n[287]\ttest-logloss:0.47187\n[288]\ttest-logloss:0.47176\n[289]\ttest-logloss:0.47175\n[290]\ttest-logloss:0.47170\n[291]\ttest-logloss:0.47167\n[292]\ttest-logloss:0.47160\n[293]\ttest-logloss:0.47145\n[294]\ttest-logloss:0.47141\n[295]\ttest-logloss:0.47139\n[296]\ttest-logloss:0.47137\n[297]\ttest-logloss:0.47125\n[298]\ttest-logloss:0.47125\n[299]\ttest-logloss:0.47124\n[300]\ttest-logloss:0.47110\n[301]\ttest-logloss:0.47109\n[302]\ttest-logloss:0.47105\n[303]\ttest-logloss:0.47104\n[304]\ttest-logloss:0.47101\n[305]\ttest-logloss:0.47100\n[306]\ttest-logloss:0.47088\n[307]\ttest-logloss:0.47079\n[308]\ttest-logloss:0.47054\n[309]\ttest-logloss:0.47034\n[310]\ttest-logloss:0.47014\n[311]\ttest-logloss:0.47009\n[312]\ttest-logloss:0.47011\n[313]\ttest-logloss:0.47012\n[314]\ttest-logloss:0.46993\n[315]\ttest-logloss:0.46982\n[316]\ttest-logloss:0.46965\n[317]\ttest-logloss:0.46951\n[318]\ttest-logloss:0.46949\n[319]\ttest-logloss:0.46931\n[320]\ttest-logloss:0.46926\n[321]\ttest-logloss:0.46923\n[322]\ttest-logloss:0.46921\n[323]\ttest-logloss:0.46921\n[324]\ttest-logloss:0.46905\n[325]\ttest-logloss:0.46905\n[326]\ttest-logloss:0.46906\n[327]\ttest-logloss:0.46905\n[328]\ttest-logloss:0.46904\n[329]\ttest-logloss:0.46905\n[330]\ttest-logloss:0.46898\n[331]\ttest-logloss:0.46893\n[332]\ttest-logloss:0.46886\n[333]\ttest-logloss:0.46882\n[334]\ttest-logloss:0.46881\n[335]\ttest-logloss:0.46856\n[336]\ttest-logloss:0.46833\n[337]\ttest-logloss:0.46833\n[338]\ttest-logloss:0.46833\n[339]\ttest-logloss:0.46818\n[340]\ttest-logloss:0.46816\n[341]\ttest-logloss:0.46816\n[342]\ttest-logloss:0.46816\n[343]\ttest-logloss:0.46815\n[344]\ttest-logloss:0.46814\n[345]\ttest-logloss:0.46810\n[346]\ttest-logloss:0.46801\n[347]\ttest-logloss:0.46782\n[348]\ttest-logloss:0.46777\n[349]\ttest-logloss:0.46764\n[350]\ttest-logloss:0.46757\n[351]\ttest-logloss:0.46740\n[352]\ttest-logloss:0.46728\n[353]\ttest-logloss:0.46711\n[354]\ttest-logloss:0.46699\n[355]\ttest-logloss:0.46686\n[356]\ttest-logloss:0.46682\n[357]\ttest-logloss:0.46677\n[358]\ttest-logloss:0.46677\n[359]\ttest-logloss:0.46674\n[360]\ttest-logloss:0.46675\n[361]\ttest-logloss:0.46672\n[362]\ttest-logloss:0.46672\n[363]\ttest-logloss:0.46670\n[364]\ttest-logloss:0.46670\n[365]\ttest-logloss:0.46670\n[366]\ttest-logloss:0.46665\n[367]\ttest-logloss:0.46664\n[368]\ttest-logloss:0.46662\n[369]\ttest-logloss:0.46659\n[370]\ttest-logloss:0.46654\n[371]\ttest-logloss:0.46653\n[372]\ttest-logloss:0.46651\n[373]\ttest-logloss:0.46650\n[374]\ttest-logloss:0.46650\n[375]\ttest-logloss:0.46650\n[376]\ttest-logloss:0.46645\n[377]\ttest-logloss:0.46643\n[378]\ttest-logloss:0.46638\n[379]\ttest-logloss:0.46631\n[380]\ttest-logloss:0.46631\n[381]\ttest-logloss:0.46631\n[382]\ttest-logloss:0.46628\n[383]\ttest-logloss:0.46626\n[384]\ttest-logloss:0.46622\n[385]\ttest-logloss:0.46613\n[386]\ttest-logloss:0.46600\n[387]\ttest-logloss:0.46591\n[388]\ttest-logloss:0.46588\n[389]\ttest-logloss:0.46587\n[390]\ttest-logloss:0.46583\n[391]\ttest-logloss:0.46585\n[392]\ttest-logloss:0.46584\n[393]\ttest-logloss:0.46574\n[394]\ttest-logloss:0.46573\n[395]\ttest-logloss:0.46576\n[396]\ttest-logloss:0.46572\n[397]\ttest-logloss:0.46571\n[398]\ttest-logloss:0.46572\n[399]\ttest-logloss:0.46572\n[400]\ttest-logloss:0.46568\n[401]\ttest-logloss:0.46562\n[402]\ttest-logloss:0.46560\n[403]\ttest-logloss:0.46555\n[404]\ttest-logloss:0.46550\n[405]\ttest-logloss:0.46548\n[406]\ttest-logloss:0.46547\n[407]\ttest-logloss:0.46542\n[408]\ttest-logloss:0.46538\n[409]\ttest-logloss:0.46530\n[410]\ttest-logloss:0.46523\n[411]\ttest-logloss:0.46521\n[412]\ttest-logloss:0.46517\n[413]\ttest-logloss:0.46509\n[414]\ttest-logloss:0.46503\n[415]\ttest-logloss:0.46497\n[416]\ttest-logloss:0.46493\n[417]\ttest-logloss:0.46490\n[418]\ttest-logloss:0.46486\n[419]\ttest-logloss:0.46483\n[420]\ttest-logloss:0.46482\n[421]\ttest-logloss:0.46481\n[422]\ttest-logloss:0.46480\n[423]\ttest-logloss:0.46473\n[424]\ttest-logloss:0.46473\n[425]\ttest-logloss:0.46463\n[426]\ttest-logloss:0.46459\n[427]\ttest-logloss:0.46457\n[428]\ttest-logloss:0.46449\n[429]\ttest-logloss:0.46445\n[430]\ttest-logloss:0.46433\n[431]\ttest-logloss:0.46429\n[432]\ttest-logloss:0.46429\n[433]\ttest-logloss:0.46429\n[434]\ttest-logloss:0.46419\n[435]\ttest-logloss:0.46419\n[436]\ttest-logloss:0.46418\n[437]\ttest-logloss:0.46419\n[438]\ttest-logloss:0.46415\n[439]\ttest-logloss:0.46408\n[440]\ttest-logloss:0.46405\n[441]\ttest-logloss:0.46398\n[442]\ttest-logloss:0.46390\n[443]\ttest-logloss:0.46377\n[444]\ttest-logloss:0.46375\n[445]\ttest-logloss:0.46374\n[446]\ttest-logloss:0.46372\n[447]\ttest-logloss:0.46369\n[448]\ttest-logloss:0.46368\n[449]\ttest-logloss:0.46369\n[450]\ttest-logloss:0.46366\n[451]\ttest-logloss:0.46363\n[452]\ttest-logloss:0.46362\n[453]\ttest-logloss:0.46353\n[454]\ttest-logloss:0.46350\n[455]\ttest-logloss:0.46347\n[456]\ttest-logloss:0.46347\n[457]\ttest-logloss:0.46347\n[458]\ttest-logloss:0.46339\n[459]\ttest-logloss:0.46337\n[460]\ttest-logloss:0.46337\n[461]\ttest-logloss:0.46336\n[462]\ttest-logloss:0.46332\n[463]\ttest-logloss:0.46331\n[464]\ttest-logloss:0.46328\n[465]\ttest-logloss:0.46323\n[466]\ttest-logloss:0.46320\n[467]\ttest-logloss:0.46320\n[468]\ttest-logloss:0.46316\n[469]\ttest-logloss:0.46306\n[470]\ttest-logloss:0.46301\n[471]\ttest-logloss:0.46298\n[472]\ttest-logloss:0.46296\n[473]\ttest-logloss:0.46294\n[474]\ttest-logloss:0.46278\n[475]\ttest-logloss:0.46276\n[476]\ttest-logloss:0.46269\n[477]\ttest-logloss:0.46253\n[478]\ttest-logloss:0.46249\n[479]\ttest-logloss:0.46240\n[480]\ttest-logloss:0.46238\n[481]\ttest-logloss:0.46235\n[482]\ttest-logloss:0.46235\n[483]\ttest-logloss:0.46231\n[484]\ttest-logloss:0.46228\n[485]\ttest-logloss:0.46222\n[486]\ttest-logloss:0.46225\n[487]\ttest-logloss:0.46219\n[488]\ttest-logloss:0.46209\n[489]\ttest-logloss:0.46206\n[490]\ttest-logloss:0.46201\n[491]\ttest-logloss:0.46193\n[492]\ttest-logloss:0.46195\n[493]\ttest-logloss:0.46194\n[494]\ttest-logloss:0.46194\n[495]\ttest-logloss:0.46189\n[496]\ttest-logloss:0.46189\n[497]\ttest-logloss:0.46188\n[498]\ttest-logloss:0.46187\n[499]\ttest-logloss:0.46184\n[500]\ttest-logloss:0.46170\n[501]\ttest-logloss:0.46161\n[502]\ttest-logloss:0.46157\n[503]\ttest-logloss:0.46156\n[504]\ttest-logloss:0.46150\n[505]\ttest-logloss:0.46149\n[506]\ttest-logloss:0.46141\n[507]\ttest-logloss:0.46136\n[508]\ttest-logloss:0.46136\n[509]\ttest-logloss:0.46130\n[510]\ttest-logloss:0.46129\n[511]\ttest-logloss:0.46124\n[512]\ttest-logloss:0.46119\n[513]\ttest-logloss:0.46119\n[514]\ttest-logloss:0.46115\n[515]\ttest-logloss:0.46107\n[516]\ttest-logloss:0.46106\n[517]\ttest-logloss:0.46100\n[518]\ttest-logloss:0.46094\n[519]\ttest-logloss:0.46091\n[520]\ttest-logloss:0.46087\n[521]\ttest-logloss:0.46087\n[522]\ttest-logloss:0.46088\n[523]\ttest-logloss:0.46090\n[524]\ttest-logloss:0.46090\n[525]\ttest-logloss:0.46090\n[526]\ttest-logloss:0.46088\n[527]\ttest-logloss:0.46088\n[528]\ttest-logloss:0.46078\n[529]\ttest-logloss:0.46068\n[530]\ttest-logloss:0.46064\n[531]\ttest-logloss:0.46057\n[532]\ttest-logloss:0.46048\n[533]\ttest-logloss:0.46046\n[534]\ttest-logloss:0.46039\n[535]\ttest-logloss:0.46038\n[536]\ttest-logloss:0.46034\n[537]\ttest-logloss:0.46027\n[538]\ttest-logloss:0.46023\n[539]\ttest-logloss:0.46020\n[540]\ttest-logloss:0.46017\n[541]\ttest-logloss:0.46012\n[542]\ttest-logloss:0.46016\n[543]\ttest-logloss:0.46013\n[544]\ttest-logloss:0.46006\n[545]\ttest-logloss:0.46009\n[546]\ttest-logloss:0.46005\n[547]\ttest-logloss:0.46004\n[548]\ttest-logloss:0.46000\n[549]\ttest-logloss:0.45999\n[550]\ttest-logloss:0.45999\n[551]\ttest-logloss:0.45999\n[552]\ttest-logloss:0.46001\n[553]\ttest-logloss:0.45999\n[554]\ttest-logloss:0.45999\n[555]\ttest-logloss:0.45996\n[556]\ttest-logloss:0.45987\n[557]\ttest-logloss:0.45980\n[558]\ttest-logloss:0.45972\n[559]\ttest-logloss:0.45969\n[560]\ttest-logloss:0.45965\n[561]\ttest-logloss:0.45963\n[562]\ttest-logloss:0.45955\n[563]\ttest-logloss:0.45946\n[564]\ttest-logloss:0.45935\n[565]\ttest-logloss:0.45918\n[566]\ttest-logloss:0.45914\n[567]\ttest-logloss:0.45911\n[568]\ttest-logloss:0.45904\n[569]\ttest-logloss:0.45900\n[570]\ttest-logloss:0.45900\n[571]\ttest-logloss:0.45896\n[572]\ttest-logloss:0.45897\n[573]\ttest-logloss:0.45897\n[574]\ttest-logloss:0.45893\n[575]\ttest-logloss:0.45885\n[576]\ttest-logloss:0.45877\n[577]\ttest-logloss:0.45869\n[578]\ttest-logloss:0.45867\n[579]\ttest-logloss:0.45858\n[580]\ttest-logloss:0.45847\n[581]\ttest-logloss:0.45847\n[582]\ttest-logloss:0.45837\n[583]\ttest-logloss:0.45831\n[584]\ttest-logloss:0.45832\n[585]\ttest-logloss:0.45832\n[586]\ttest-logloss:0.45824\n[587]\ttest-logloss:0.45822\n[588]\ttest-logloss:0.45819\n[589]\ttest-logloss:0.45817\n[590]\ttest-logloss:0.45818\n[591]\ttest-logloss:0.45815\n[592]\ttest-logloss:0.45811\n[593]\ttest-logloss:0.45808\n[594]\ttest-logloss:0.45805\n[595]\ttest-logloss:0.45804\n[596]\ttest-logloss:0.45804\n[597]\ttest-logloss:0.45804\n[598]\ttest-logloss:0.45799\n[599]\ttest-logloss:0.45797\n[600]\ttest-logloss:0.45790\n[601]\ttest-logloss:0.45786\n[602]\ttest-logloss:0.45786\n[603]\ttest-logloss:0.45787\n[604]\ttest-logloss:0.45784\n[605]\ttest-logloss:0.45778\n[606]\ttest-logloss:0.45778\n[607]\ttest-logloss:0.45775\n[608]\ttest-logloss:0.45766\n[609]\ttest-logloss:0.45757\n[610]\ttest-logloss:0.45752\n[611]\ttest-logloss:0.45751\n[612]\ttest-logloss:0.45752\n[613]\ttest-logloss:0.45751\n[614]\ttest-logloss:0.45751\n[615]\ttest-logloss:0.45751\n[616]\ttest-logloss:0.45751\n[617]\ttest-logloss:0.45753\n[618]\ttest-logloss:0.45753\n[619]\ttest-logloss:0.45751\n[620]\ttest-logloss:0.45752\n[621]\ttest-logloss:0.45749\n[622]\ttest-logloss:0.45749\n[623]\ttest-logloss:0.45745\n[624]\ttest-logloss:0.45740\n[625]\ttest-logloss:0.45730\n[626]\ttest-logloss:0.45733\n[627]\ttest-logloss:0.45729\n[628]\ttest-logloss:0.45728\n[629]\ttest-logloss:0.45730\n[630]\ttest-logloss:0.45713\n[631]\ttest-logloss:0.45710\n[632]\ttest-logloss:0.45705\n[633]\ttest-logloss:0.45703\n[634]\ttest-logloss:0.45703\n[635]\ttest-logloss:0.45702\n[636]\ttest-logloss:0.45697\n[637]\ttest-logloss:0.45698\n[638]\ttest-logloss:0.45696\n[639]\ttest-logloss:0.45697\n[640]\ttest-logloss:0.45695\n[641]\ttest-logloss:0.45694\n[642]\ttest-logloss:0.45695\n[643]\ttest-logloss:0.45694\n[644]\ttest-logloss:0.45694\n[645]\ttest-logloss:0.45694\n[646]\ttest-logloss:0.45694\n[647]\ttest-logloss:0.45693\n[648]\ttest-logloss:0.45693\n[649]\ttest-logloss:0.45691\n[650]\ttest-logloss:0.45690\n[651]\ttest-logloss:0.45690\n[652]\ttest-logloss:0.45689\n[653]\ttest-logloss:0.45688\n[654]\ttest-logloss:0.45686\n[655]\ttest-logloss:0.45686\n[656]\ttest-logloss:0.45684\n[657]\ttest-logloss:0.45682\n[658]\ttest-logloss:0.45681\n[659]\ttest-logloss:0.45676\n[660]\ttest-logloss:0.45677\n[661]\ttest-logloss:0.45677\n[662]\ttest-logloss:0.45678\n[663]\ttest-logloss:0.45677\n[664]\ttest-logloss:0.45679\n[665]\ttest-logloss:0.45680\n[666]\ttest-logloss:0.45679\n[667]\ttest-logloss:0.45677\n[668]\ttest-logloss:0.45674\n[669]\ttest-logloss:0.45674\n[670]\ttest-logloss:0.45670\n[671]\ttest-logloss:0.45665\n[672]\ttest-logloss:0.45653\n[673]\ttest-logloss:0.45654\n[674]\ttest-logloss:0.45652\n[675]\ttest-logloss:0.45650\n[676]\ttest-logloss:0.45651\n[677]\ttest-logloss:0.45650\n[678]\ttest-logloss:0.45648\n[679]\ttest-logloss:0.45646\n[680]\ttest-logloss:0.45639\n[681]\ttest-logloss:0.45640\n[682]\ttest-logloss:0.45638\n[683]\ttest-logloss:0.45637\n[684]\ttest-logloss:0.45635\n[685]\ttest-logloss:0.45635\n[686]\ttest-logloss:0.45633\n[687]\ttest-logloss:0.45630\n[688]\ttest-logloss:0.45628\n[689]\ttest-logloss:0.45626\n[690]\ttest-logloss:0.45624\n[691]\ttest-logloss:0.45623\n[692]\ttest-logloss:0.45621\n[693]\ttest-logloss:0.45623\n[694]\ttest-logloss:0.45622\n[695]\ttest-logloss:0.45618\n[696]\ttest-logloss:0.45617\n[697]\ttest-logloss:0.45612\n[698]\ttest-logloss:0.45606\n[699]\ttest-logloss:0.45597\n[700]\ttest-logloss:0.45595\n[701]\ttest-logloss:0.45594\n[702]\ttest-logloss:0.45594\n[703]\ttest-logloss:0.45586\n[704]\ttest-logloss:0.45582\n[705]\ttest-logloss:0.45571\n[706]\ttest-logloss:0.45569\n[707]\ttest-logloss:0.45559\n[708]\ttest-logloss:0.45549\n[709]\ttest-logloss:0.45549\n[710]\ttest-logloss:0.45551\n[711]\ttest-logloss:0.45552\n[712]\ttest-logloss:0.45550\n[713]\ttest-logloss:0.45551\n[714]\ttest-logloss:0.45550\n[715]\ttest-logloss:0.45548\n[716]\ttest-logloss:0.45547\n[717]\ttest-logloss:0.45547\n[718]\ttest-logloss:0.45541\n[719]\ttest-logloss:0.45540\n[720]\ttest-logloss:0.45539\n[721]\ttest-logloss:0.45534\n[722]\ttest-logloss:0.45530\n[723]\ttest-logloss:0.45528\n[724]\ttest-logloss:0.45528\n[725]\ttest-logloss:0.45529\n[726]\ttest-logloss:0.45525\n[727]\ttest-logloss:0.45531\n[728]\ttest-logloss:0.45528\n[729]\ttest-logloss:0.45524\n[730]\ttest-logloss:0.45521\n[731]\ttest-logloss:0.45520\n[732]\ttest-logloss:0.45514\n[733]\ttest-logloss:0.45510\n[734]\ttest-logloss:0.45507\n[735]\ttest-logloss:0.45505\n[736]\ttest-logloss:0.45509\n[737]\ttest-logloss:0.45506\n[738]\ttest-logloss:0.45502\n[739]\ttest-logloss:0.45489\n[740]\ttest-logloss:0.45488\n[741]\ttest-logloss:0.45486\n[742]\ttest-logloss:0.45485\n[743]\ttest-logloss:0.45490\n[744]\ttest-logloss:0.45489\n[745]\ttest-logloss:0.45487\n[746]\ttest-logloss:0.45488\n[747]\ttest-logloss:0.45484\n[748]\ttest-logloss:0.45477\n[749]\ttest-logloss:0.45471\n[750]\ttest-logloss:0.45475\n[751]\ttest-logloss:0.45472\n[752]\ttest-logloss:0.45465\n[753]\ttest-logloss:0.45464\n[754]\ttest-logloss:0.45464\n[755]\ttest-logloss:0.45464\n[756]\ttest-logloss:0.45464\n[757]\ttest-logloss:0.45463\n[758]\ttest-logloss:0.45456\n[759]\ttest-logloss:0.45445\n[760]\ttest-logloss:0.45444\n[761]\ttest-logloss:0.45441\n[762]\ttest-logloss:0.45438\n[763]\ttest-logloss:0.45437\n[764]\ttest-logloss:0.45438\n[765]\ttest-logloss:0.45439\n[766]\ttest-logloss:0.45438\n[767]\ttest-logloss:0.45434\n[768]\ttest-logloss:0.45432\n[769]\ttest-logloss:0.45432\n[770]\ttest-logloss:0.45432\n[771]\ttest-logloss:0.45430\n[772]\ttest-logloss:0.45432\n[773]\ttest-logloss:0.45432\n[774]\ttest-logloss:0.45430\n[775]\ttest-logloss:0.45421\n[776]\ttest-logloss:0.45421\n[777]\ttest-logloss:0.45420\n[778]\ttest-logloss:0.45420\n[779]\ttest-logloss:0.45417\n[780]\ttest-logloss:0.45417\n[781]\ttest-logloss:0.45416\n[782]\ttest-logloss:0.45415\n[783]\ttest-logloss:0.45414\n[784]\ttest-logloss:0.45411\n[785]\ttest-logloss:0.45411\n[786]\ttest-logloss:0.45409\n[787]\ttest-logloss:0.45406\n[788]\ttest-logloss:0.45402\n[789]\ttest-logloss:0.45401\n[790]\ttest-logloss:0.45399\n[791]\ttest-logloss:0.45394\n[792]\ttest-logloss:0.45391\n[793]\ttest-logloss:0.45389\n[794]\ttest-logloss:0.45391\n[795]\ttest-logloss:0.45387\n[796]\ttest-logloss:0.45387\n[797]\ttest-logloss:0.45388\n[798]\ttest-logloss:0.45388\n[799]\ttest-logloss:0.45388\n[800]\ttest-logloss:0.45382\n[801]\ttest-logloss:0.45383\n[802]\ttest-logloss:0.45382\n[803]\ttest-logloss:0.45383\n[804]\ttest-logloss:0.45382\n[805]\ttest-logloss:0.45383\n[806]\ttest-logloss:0.45384\n[807]\ttest-logloss:0.45383\n[808]\ttest-logloss:0.45382\n[809]\ttest-logloss:0.45380\n[810]\ttest-logloss:0.45374\n[811]\ttest-logloss:0.45375\n[812]\ttest-logloss:0.45373\n[813]\ttest-logloss:0.45374\n[814]\ttest-logloss:0.45374\n[815]\ttest-logloss:0.45366\n[816]\ttest-logloss:0.45367\n[817]\ttest-logloss:0.45364\n[818]\ttest-logloss:0.45364\n[819]\ttest-logloss:0.45364\n[820]\ttest-logloss:0.45363\n[821]\ttest-logloss:0.45363\n[822]\ttest-logloss:0.45355\n[823]\ttest-logloss:0.45348\n[824]\ttest-logloss:0.45340\n[825]\ttest-logloss:0.45336\n[826]\ttest-logloss:0.45335\n[827]\ttest-logloss:0.45331\n[828]\ttest-logloss:0.45329\n[829]\ttest-logloss:0.45328\n[830]\ttest-logloss:0.45329\n[831]\ttest-logloss:0.45324\n[832]\ttest-logloss:0.45327\n[833]\ttest-logloss:0.45324\n[834]\ttest-logloss:0.45323\n[835]\ttest-logloss:0.45323\n[836]\ttest-logloss:0.45324\n[837]\ttest-logloss:0.45322\n[838]\ttest-logloss:0.45321\n[839]\ttest-logloss:0.45320\n[840]\ttest-logloss:0.45318\n[841]\ttest-logloss:0.45318\n[842]\ttest-logloss:0.45318\n[843]\ttest-logloss:0.45316\n[844]\ttest-logloss:0.45314\n[845]\ttest-logloss:0.45314\n[846]\ttest-logloss:0.45311\n[847]\ttest-logloss:0.45312\n[848]\ttest-logloss:0.45308\n[849]\ttest-logloss:0.45306\n[850]\ttest-logloss:0.45305\n[851]\ttest-logloss:0.45304\n[852]\ttest-logloss:0.45301\n[853]\ttest-logloss:0.45301\n[854]\ttest-logloss:0.45298\n[855]\ttest-logloss:0.45294\n[856]\ttest-logloss:0.45290\n[857]\ttest-logloss:0.45290\n[858]\ttest-logloss:0.45289\n[859]\ttest-logloss:0.45290\n[860]\ttest-logloss:0.45291\n[861]\ttest-logloss:0.45287\n[862]\ttest-logloss:0.45285\n[863]\ttest-logloss:0.45286\n[864]\ttest-logloss:0.45285\n[865]\ttest-logloss:0.45283\n[866]\ttest-logloss:0.45283\n[867]\ttest-logloss:0.45283\n[868]\ttest-logloss:0.45284\n[869]\ttest-logloss:0.45284\n[870]\ttest-logloss:0.45280\n[871]\ttest-logloss:0.45274\n[872]\ttest-logloss:0.45273\n[873]\ttest-logloss:0.45267\n[874]\ttest-logloss:0.45266\n[875]\ttest-logloss:0.45265\n[876]\ttest-logloss:0.45265\n[877]\ttest-logloss:0.45265\n[878]\ttest-logloss:0.45265\n[879]\ttest-logloss:0.45265\n[880]\ttest-logloss:0.45264\n[881]\ttest-logloss:0.45265\n[882]\ttest-logloss:0.45265\n[883]\ttest-logloss:0.45266\n[884]\ttest-logloss:0.45266\n[885]\ttest-logloss:0.45267\n[886]\ttest-logloss:0.45267\n[887]\ttest-logloss:0.45267\n[888]\ttest-logloss:0.45267\n[889]\ttest-logloss:0.45266\n[890]\ttest-logloss:0.45267\n[891]\ttest-logloss:0.45268\n[892]\ttest-logloss:0.45267\n[893]\ttest-logloss:0.45267\n[894]\ttest-logloss:0.45265\n[895]\ttest-logloss:0.45265\n[896]\ttest-logloss:0.45265\n[897]\ttest-logloss:0.45265\n[898]\ttest-logloss:0.45265\n[899]\ttest-logloss:0.45265\n[900]\ttest-logloss:0.45264\n[901]\ttest-logloss:0.45264\n[902]\ttest-logloss:0.45267\n[903]\ttest-logloss:0.45260\n[904]\ttest-logloss:0.45257\n[905]\ttest-logloss:0.45254\n[906]\ttest-logloss:0.45255\n[907]\ttest-logloss:0.45254\n[908]\ttest-logloss:0.45252\n[909]\ttest-logloss:0.45247\n[910]\ttest-logloss:0.45246\n[911]\ttest-logloss:0.45244\n[912]\ttest-logloss:0.45240\n[913]\ttest-logloss:0.45235\n[914]\ttest-logloss:0.45234\n[915]\ttest-logloss:0.45232\n[916]\ttest-logloss:0.45233\n[917]\ttest-logloss:0.45233\n[918]\ttest-logloss:0.45232\n[919]\ttest-logloss:0.45229\n[920]\ttest-logloss:0.45230\n[921]\ttest-logloss:0.45229\n[922]\ttest-logloss:0.45231\n[923]\ttest-logloss:0.45231\n[924]\ttest-logloss:0.45224\n[925]\ttest-logloss:0.45213\n[926]\ttest-logloss:0.45205\n[927]\ttest-logloss:0.45200\n[928]\ttest-logloss:0.45199\n[929]\ttest-logloss:0.45193\n[930]\ttest-logloss:0.45189\n[931]\ttest-logloss:0.45184\n[932]\ttest-logloss:0.45184\n[933]\ttest-logloss:0.45181\n[934]\ttest-logloss:0.45181\n[935]\ttest-logloss:0.45176\n[936]\ttest-logloss:0.45174\n[937]\ttest-logloss:0.45170\n[938]\ttest-logloss:0.45172\n[939]\ttest-logloss:0.45173\n[940]\ttest-logloss:0.45173\n[941]\ttest-logloss:0.45168\n[942]\ttest-logloss:0.45169\n[943]\ttest-logloss:0.45167\n[944]\ttest-logloss:0.45165\n[945]\ttest-logloss:0.45161\n[946]\ttest-logloss:0.45156\n[947]\ttest-logloss:0.45152\n[948]\ttest-logloss:0.45151\n[949]\ttest-logloss:0.45151\n[950]\ttest-logloss:0.45147\n[951]\ttest-logloss:0.45146\n[952]\ttest-logloss:0.45146\n[953]\ttest-logloss:0.45141\n[954]\ttest-logloss:0.45141\n[955]\ttest-logloss:0.45137\n[956]\ttest-logloss:0.45132\n[957]\ttest-logloss:0.45134\n[958]\ttest-logloss:0.45131\n[959]\ttest-logloss:0.45127\n[960]\ttest-logloss:0.45125\n[961]\ttest-logloss:0.45121\n[962]\ttest-logloss:0.45115\n[963]\ttest-logloss:0.45115\n[964]\ttest-logloss:0.45112\n[965]\ttest-logloss:0.45114\n[966]\ttest-logloss:0.45110\n[967]\ttest-logloss:0.45111\n[968]\ttest-logloss:0.45107\n[969]\ttest-logloss:0.45102\n[970]\ttest-logloss:0.45102\n[971]\ttest-logloss:0.45104\n[972]\ttest-logloss:0.45100\n[973]\ttest-logloss:0.45101\n[974]\ttest-logloss:0.45101\n[975]\ttest-logloss:0.45099\n[976]\ttest-logloss:0.45098\n[977]\ttest-logloss:0.45098\n[978]\ttest-logloss:0.45092\n[979]\ttest-logloss:0.45089\n[980]\ttest-logloss:0.45081\n[981]\ttest-logloss:0.45080\n[982]\ttest-logloss:0.45080\n[983]\ttest-logloss:0.45077\n[984]\ttest-logloss:0.45075\n[985]\ttest-logloss:0.45074\n[986]\ttest-logloss:0.45072\n[987]\ttest-logloss:0.45072\n[988]\ttest-logloss:0.45068\n[989]\ttest-logloss:0.45068\n[990]\ttest-logloss:0.45067\n[991]\ttest-logloss:0.45068\n[992]\ttest-logloss:0.45065\n[993]\ttest-logloss:0.45064\n[994]\ttest-logloss:0.45060\n[995]\ttest-logloss:0.45060\n[996]\ttest-logloss:0.45056\n[997]\ttest-logloss:0.45055\n[998]\ttest-logloss:0.45052\n[999]\ttest-logloss:0.45047\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/10/15 13:13:44 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-af8147ed-d940-457c-bb64-6624008909b1/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:13:43] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\"\n"
     ]
    }
   ],
   "source": [
    "mlflow.xgboost.autolog()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    train = xgb.DMatrix(data = X_train_processed, label = y_train)\n",
    "    test = xgb.DMatrix(data = X_test_processed, label = y_test)\n",
    "    \n",
    "    booster = xgb.train(params = param, dtrain = train, num_boost_round = 1000,\\\n",
    "                        evals = [(test, 'test')], early_stopping_rounds = 50)\n",
    "    \n",
    "    predictions_test = booster.predict(test)\n",
    "    y_pred = [round(value) for value in predictions_test]\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_precision_score = precision_score(y_test, y_pred)\n",
    "    test_recall_score = recall_score(y_test, y_pred)\n",
    "    test_f1_score = f1_score(y_test, y_pred)\n",
    "    auc_score = roc_auc_score(y_test, predictions_test)\n",
    "        \n",
    "    mlflow.log_metric('test_accuracy',  test_accuracy)\n",
    "    mlflow.log_metric('test_precision', test_precision_score)\n",
    "    mlflow.log_metric('test_recall', test_recall_score)\n",
    "    mlflow.log_metric('test_f1', test_f1_score)\n",
    "    mlflow.log_metric('AUC_score', auc_score)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65559587-72fb-4ce4-a082-2b15f8c19d67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ClassificationUsingXGBoost",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
