{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "def33889-163f-4278-b519-4acecea2aba3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a239893-a4c3-4d48-b52a-d05ac38f9183",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Loading the data\n",
    "Displaying  the first 5 rows of the dataset\n",
    "link=-https://www.kaggle.com/santoshd3/bank-customers?select=Churn+Modeling.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9455a224-0fea-473f-8ebd-234ef9307aa3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CustomerId</th>\n      <th>Surname</th>\n      <th>CreditScore</th>\n      <th>Geography</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>Balance</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>EstimatedSalary</th>\n      <th>Exited</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15634602</td>\n      <td>Hargrave</td>\n      <td>619</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>2</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>101348.88</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>15647311</td>\n      <td>Hill</td>\n      <td>608</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>41</td>\n      <td>1</td>\n      <td>83807.86</td>\n      <td>1</td>\n      <td>No</td>\n      <td>Yes</td>\n      <td>112542.58</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>15619304</td>\n      <td>Onio</td>\n      <td>502</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>8</td>\n      <td>159660.80</td>\n      <td>3</td>\n      <td>Yes</td>\n      <td>No</td>\n      <td>113931.57</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>15701354</td>\n      <td>Boni</td>\n      <td>699</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>39</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>No</td>\n      <td>No</td>\n      <td>93826.63</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>15737888</td>\n      <td>Mitchell</td>\n      <td>850</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>43</td>\n      <td>2</td>\n      <td>125510.82</td>\n      <td>1</td>\n      <td>Yes</td>\n      <td>Yes</td>\n      <td>79084.10</td>\n      <td>No</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cust_attrition_data = pd.read_csv('/dbfs/FileStore/tables/Bank_customer.csv')\n",
    "\n",
    "cust_attrition_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f33ec5e3-c9c2-45e2-9003-7d3e850a1cd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: (10000, 13)"
     ]
    }
   ],
   "source": [
    "cust_attrition_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "381d93c7-e112-42e6-b0a4-86f6e099f7b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: Index(['CustomerId', 'Surname', 'CreditScore', 'Geography', 'Gender', 'Age',\n       'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember',\n       'EstimatedSalary', 'Exited'],\n      dtype='object')"
     ]
    }
   ],
   "source": [
    "cust_attrition_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08b4c5a4-b2a9-4a02-a26b-936d77ba4de3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Checking unique values for each column.Customerid and Surname can be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15562c47-5e5d-4d6b-9eac-bf72bc2e7102",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: CustomerId         10000\nSurname             2932\nCreditScore          460\nGeography              3\nGender                 3\nAge                   70\nTenure                11\nBalance             6382\nNumOfProducts          4\nHasCrCard              2\nIsActiveMember         2\nEstimatedSalary     9999\nExited                 2\ndtype: int64"
     ]
    }
   ],
   "source": [
    "cust_attrition_data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "579fccaa-d5dd-4d3f-a34c-3eeb60c85aa4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Features data and target data are made into separate dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53f60694-3143-4d4f-92de-53df218917f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X = cust_attrition_data.drop(['CustomerId', 'Surname', 'Exited'], axis = 1)\n",
    "y = cust_attrition_data['Exited']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4918f8ff-5d67-4a3a-818f-e5c78e402c2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Target variable is converted into numeric form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07980fa6-e432-446d-8bc3-06fdcd5307cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: 0       1\n1       0\n2       1\n3       0\n4       0\n       ..\n9995    0\n9996    0\n9997    1\n9998    1\n9999    0\nName: Exited, Length: 10000, dtype: int64"
     ]
    }
   ],
   "source": [
    "class_dict = {'No': 0, 'Yes': 1}\n",
    "\n",
    "y = y.replace(class_dict)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4ffb442-be88-4e3e-95d9-a95b3b7aa98a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Data is imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "687c742e-bac9-4e03-9854-38390adcfca3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: 0    7963\n1    2037\nName: Exited, dtype: int64"
     ]
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cf5a2b1-4849-4a6c-bbd5-32601d20b5f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We are going to one hot encode categorical columns with dropping first category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69449ef0-3792-47f7-8b87-4f7b915472f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "categoricalCols = ['Geography', 'Gender', 'HasCrCard', 'IsActiveMember']\n",
    "numericCols = ['Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
    "\n",
    "categorical_transformer = OneHotEncoder(drop = 'first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe3b2757-213c-416f-a1e7-6cbcb00a6016",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Preprocessing steps are defined with ColumnTransformer which will one hot encode only categorical colums and scale remainder numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bde357a-2e98-4dbb-a691-19150b559173",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColumnTransformer(remainder=StandardScaler(),\n                  transformers=[('cat', OneHotEncoder(drop='first'),\n                                 ['Geography', 'Gender', 'HasCrCard',\n                                  'IsActiveMember'])])\n"
     ]
    }
   ],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [('cat', categorical_transformer, categoricalCols)], \n",
    "    remainder = StandardScaler() \n",
    ")\n",
    "\n",
    "print(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67901c68-b332-443a-ba65-4f37fab76644",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: ((7000, 10), (3000, 10))"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b16c164-0fef-4336-b5a4-f08699b87a69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Creating and train a decision tree classifier model and by using MLFLOW we are logging \n",
    " parameters, metrics, and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1d275da-c222-485f-a3b1-bde128294951",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    " \n",
    "    criterion= 'entropy'\n",
    "    max_depth = 5\n",
    "    max_features = 5\n",
    "    \n",
    "    dtc = DecisionTreeClassifier(criterion = criterion, max_depth = max_depth,\n",
    "                                 max_features = max_features)\n",
    "\n",
    "    pipeline = Pipeline( steps = [('preprocessor', preprocessor), ('classifier', dtc)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    predictions =  pipeline.predict(X_test) \n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precisionscore = precision_score(y_test, predictions)\n",
    "    recallscore = recall_score(y_test, predictions)\n",
    "    f1score = f1_score(y_test, predictions)\n",
    "  \n",
    "    mlflow.log_param('criterion', criterion)\n",
    "    mlflow.log_param('max_depth', max_depth)\n",
    "    mlflow.log_param('max_features', max_features)\n",
    "  \n",
    "    mlflow.log_metric('F1score',  f1score)\n",
    "    mlflow.log_metric('Recall_score',  recallscore)\n",
    "    mlflow.log_metric('Precision_score',  precisionscore)\n",
    "    mlflow.log_metric('Accuracy_score',  accuracy)\n",
    "\n",
    "    mlflow.sklearn.log_model(dtc, 'dtc_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bce1103e-9f90-461d-8d3b-5e4458b616f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt.pyll import scope\n",
    "\n",
    "def objective(params):\n",
    "    \n",
    "    with mlflow.start_run(nested = True):\n",
    "\n",
    "        clf = DecisionTreeClassifier(max_features = params['max_features'], \n",
    "                                     max_depth = params['max_depth'],\n",
    "                                     criterion = params['criterion'])\n",
    "        pipeline = Pipeline(steps = [('preprocessor', preprocessor), ('model', clf)])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        predictions =  pipeline.predict(X_test) \n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        precisionscore = precision_score(y_test, predictions)\n",
    "        recallscore = recall_score(y_test, predictions)\n",
    "        f1score = f1_score(y_test, predictions)\n",
    "\n",
    "        mlflow.log_metric('F1score',  f1score)\n",
    "        mlflow.log_metric('Recall_score',  recallscore)\n",
    "        mlflow.log_metric('Precision_score',  precisionscore)\n",
    "        mlflow.log_metric('Accuracy_score',  accuracy)\n",
    "\n",
    "        mlflow.sklearn.log_model(dtc, 'dtc_hpo')\n",
    "  \n",
    "        return {'loss': -f1score, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "186286b5-bf72-40ff-8b60-b5b30d6a80ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Search space for hyperparameters is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f49ab54f-c24a-4dae-a163-9f6e10dc3d38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "search_space = {'max_features': scope.int(hp.quniform('max_features', 1, 10, 1)),\n",
    "                'max_depth': scope.int(hp.quniform('max_depth', 1, 15, 1)),\n",
    "                'criterion': hp.choice('criterion', ['gini', 'entropy'])} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53304429-c81a-491b-97e0-97a9fd08f3e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Algorithm is defined\n",
    "\n",
    "The two main choices are:\n",
    "\n",
    "hyperopt.tpe.suggest: Tree of Parzen Estimators, a Bayesian approach which iteratively and adaptively selects new hyperparameter settings to explore based on past results\n",
    "hyperopt.rand.suggest: Random search, a non-adaptive approach that samples over the search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8a6905c-9f3a-4e22-9b97-491871b3fab3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "algo = tpe.suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11015f83-309c-4152-8146-3d227e38ad44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/16 [00:00<?, ?trial/s, best loss=?]\r  6%|▋         | 1/16 [00:04<01:08,  4.59s/trial, best loss: -0.4413239719157473]\r 12%|█▎        | 2/16 [00:08<01:00,  4.33s/trial, best loss: -0.5450819672131146]\r 19%|█▉        | 3/16 [00:12<00:53,  4.11s/trial, best loss: -0.5450819672131146]\r 25%|██▌       | 4/16 [00:16<00:48,  4.01s/trial, best loss: -0.5450819672131146]\r 31%|███▏      | 5/16 [00:20<00:44,  4.07s/trial, best loss: -0.5540166204986151]\r 38%|███▊      | 6/16 [00:24<00:40,  4.02s/trial, best loss: -0.5540166204986151]\r 44%|████▍     | 7/16 [00:28<00:35,  3.95s/trial, best loss: -0.5540166204986151]\r 50%|█████     | 8/16 [00:32<00:31,  3.94s/trial, best loss: -0.5540166204986151]\r 56%|█████▋    | 9/16 [00:36<00:27,  3.91s/trial, best loss: -0.5540166204986151]\r 62%|██████▎   | 10/16 [00:39<00:23,  3.88s/trial, best loss: -0.5540166204986151]\r 69%|██████▉   | 11/16 [00:43<00:19,  3.88s/trial, best loss: -0.5540166204986151]\r 75%|███████▌  | 12/16 [00:47<00:15,  3.86s/trial, best loss: -0.5540166204986151]\r 81%|████████▏ | 13/16 [00:51<00:11,  3.92s/trial, best loss: -0.5540166204986151]\r 88%|████████▊ | 14/16 [00:55<00:07,  3.91s/trial, best loss: -0.5540166204986151]\r 94%|█████████▍| 15/16 [00:59<00:03,  3.92s/trial, best loss: -0.5540166204986151]\r100%|██████████| 16/16 [01:03<00:00,  3.92s/trial, best loss: -0.5540166204986151]\r100%|██████████| 16/16 [01:03<00:00,  3.96s/trial, best loss: -0.5540166204986151]\nBest value found:  {'criterion': 0, 'max_depth': 9.0, 'max_features': 9.0}\n"
     ]
    }
   ],
   "source": [
    "argmin = fmin(\n",
    "  fn = objective,\n",
    "  space = search_space,\n",
    "  algo = algo,\n",
    "  max_evals = 16)\n",
    "\n",
    "print('Best value found: ', argmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beedbfe9-81d1-407a-9fb2-4ed4c716834b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Because the requested parallelism was None or a non-positive value, parallelism will be set to (4), which is Spark's default parallelism (4), or 1, whichever is greater. We recommend setting parallelism explicitly to a positive value because the total of Spark task slots is subject to cluster sizing.\nHyperopt with SparkTrials will automatically track trials in MLflow. To view the MLflow experiment associated with the notebook, click the 'Runs' icon in the notebook context bar on the upper right. There, you can view all runs.\nTo view logs from trials, please check the Spark executor logs. To view executor logs, expand 'Spark Jobs' above until you see the (i) icon next to the stage from the trial job. Click it and find the list of tasks. Click the 'stderr' link for a task to view trial logs.\n\r  0%|          | 0/16 [00:00<?, ?trial/s, best loss=?]\r  6%|▋         | 1/16 [00:11<02:47, 11.15s/trial, best loss: -0.5822784810126582]\r 12%|█▎        | 2/16 [00:12<01:12,  5.18s/trial, best loss: -0.5822784810126582]\r 19%|█▉        | 3/16 [00:13<00:42,  3.28s/trial, best loss: -0.5822784810126582]\r 25%|██▌       | 4/16 [00:14<00:28,  2.38s/trial, best loss: -0.5822784810126582]\r 38%|███▊      | 6/16 [00:21<00:29,  2.98s/trial, best loss: -0.5822784810126582]\r 50%|█████     | 8/16 [00:23<00:17,  2.13s/trial, best loss: -0.5822784810126582]\r 56%|█████▋    | 9/16 [00:30<00:22,  3.28s/trial, best loss: -0.5822784810126582]\r 62%|██████▎   | 10/16 [00:31<00:16,  2.71s/trial, best loss: -0.5822784810126582]\r 69%|██████▉   | 11/16 [00:32<00:11,  2.26s/trial, best loss: -0.5822784810126582]\r 75%|███████▌  | 12/16 [00:33<00:07,  1.91s/trial, best loss: -0.5822784810126582]\r 81%|████████▏ | 13/16 [00:40<00:10,  3.35s/trial, best loss: -0.5822784810126582]\r 88%|████████▊ | 14/16 [00:41<00:05,  2.68s/trial, best loss: -0.5822784810126582]\r 94%|█████████▍| 15/16 [00:42<00:02,  2.19s/trial, best loss: -0.5822784810126582]\r100%|██████████| 16/16 [00:44<00:00,  2.14s/trial, best loss: -0.5822784810126582]\r100%|██████████| 16/16 [00:44<00:00,  2.77s/trial, best loss: -0.5822784810126582]\nTotal Trials: 16: 16 succeeded, 0 failed, 0 cancelled.\nBest value found:  {'criterion': 0, 'max_depth': 7.0, 'max_features': 10.0}\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import SparkTrials\n",
    " \n",
    "spark_trials = SparkTrials()\n",
    " \n",
    "with mlflow.start_run():\n",
    "    argmin = fmin(\n",
    "    fn = objective,\n",
    "    space = search_space,\n",
    "    algo = algo,\n",
    "    max_evals = 16,\n",
    "    trials = spark_trials)\n",
    "\n",
    "print('Best value found: ', argmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1324448d-ec66-4166-b918-53d83843c9a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: [{'loss': -0.5822784810126582, 'status': 'ok'},\n {'loss': -0.5004721435316336, 'status': 'ok'},\n {'loss': -0.0, 'status': 'ok'},\n {'loss': -0.5549845837615621, 'status': 'ok'},\n {'loss': -0.45837615621788286, 'status': 'ok'},\n {'loss': -0.33834586466165417, 'status': 'ok'},\n {'loss': -0.3480278422273782, 'status': 'ok'},\n {'loss': -0.48462929475587707, 'status': 'ok'},\n {'loss': -0.5256673511293634, 'status': 'ok'},\n {'loss': -0.5088757396449705, 'status': 'ok'},\n {'loss': -0.0, 'status': 'ok'},\n {'loss': -0.526810912511759, 'status': 'ok'},\n {'loss': -0.27114093959731544, 'status': 'ok'},\n {'loss': -0.51295799821269, 'status': 'ok'},\n {'loss': -0.5483568075117371, 'status': 'ok'},\n {'loss': -0.52565445026178, 'status': 'ok'}]"
     ]
    }
   ],
   "source": [
    "spark_trials.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "961c632d-fef3-4c6b-a788-7cc4e5a90c2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now, we examine four algorithms available in scikit-learn: support vector machines (SVM), random forest, and logistic regression and Decison tree\n",
    "\n",
    "In the following cell, we are defining  a parameter params['type'] for the model name. This function also runs the training and calculates the cross-validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc859cf3-5aa9-44de-94c7-de551f06a48a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "\n",
    "    with mlflow.start_run(nested = True):\n",
    "        classifier_type = params['type']\n",
    "        del params['type']\n",
    "        \n",
    "        if classifier_type == 'svm':\n",
    "            clf = SVC(**params)\n",
    "        elif classifier_type == 'rf':\n",
    "            clf = RandomForestClassifier(**params)\n",
    "        elif classifier_type == 'logreg':\n",
    "            clf = LogisticRegression(**params)\n",
    "        elif classifier_type == 'dtc':\n",
    "            clf = DecisionTreeClassifier(**params)\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "        pipeline = Pipeline(steps = [('preprocessor', preprocessor), ('model', clf)])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        predictions =  pipeline.predict(X_test) \n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        precisionscore = precision_score(y_test, predictions)\n",
    "        recallscore = recall_score(y_test, predictions)\n",
    "        f1score = f1_score(y_test, predictions)\n",
    "        \n",
    "        mlflow.log_metric('F1score',  f1score)\n",
    "        mlflow.log_metric('Recall_score',  recallscore)\n",
    "        mlflow.log_metric('Precision_score',  precisionscore)\n",
    "        mlflow.log_metric('Accuracy_score',  accuracy)\n",
    "        \n",
    "        mlflow.sklearn.log_model(clf, 'clf_hpo')\n",
    "\n",
    "        return {'loss': -f1score, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf70fd01-5059-4d6b-8d28-4e8a18e25a7a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Search space is defined for multiple models\n",
    "Using hp.choice to select different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71b4ce53-37c5-468e-98ef-b0e7ba299cce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "search_space = hp.choice('classifier_type', [\n",
    "    {\n",
    "        'type': 'svm',\n",
    "        'C': hp.lognormal('SVM_C', 0, 1.0),\n",
    "        'kernel': hp.choice('kernel', ['linear', 'rbf'])\n",
    "    },\n",
    "    {\n",
    "        'type': 'rf',\n",
    "        'n_estimators':scope.int(hp.quniform('n_estimators', 100, 500, 50)),\n",
    "        'max_depth': scope.int(hp.quniform('max_depth_rf', 2, 20 , 1)),\n",
    "        'criterion': hp.choice('criterion_rf', ['gini', 'entropy'])\n",
    "    },\n",
    "    {\n",
    "        'type': 'logreg',\n",
    "        'C': hp.lognormal('LR_C', 0, 1.0),\n",
    "        'solver': hp.choice('solver', ['liblinear', 'lbfgs'])\n",
    "    },\n",
    "  \n",
    "    {\n",
    "        'type': 'dtc',\n",
    "        'max_features':scope.int(hp.quniform('max_features', 1,10,1)),\n",
    "        'max_depth': scope.int(hp.quniform('max_depth_dtc', 2, 20, 1)),\n",
    "        'criterion': hp.choice('criterion_dtc', ['gini', 'entropy'])\n",
    "    }\n",
    "    \n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b767363d-c818-4413-b49f-454373cb6ccf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Same steps are repeated as done for single model case. This time Best accuracy obtained is around 86% with RF model with hyperparameters {'criterion': 'gini', 'max_depth': 11, 'n_estimators': 100, 'type': 'rf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54224611-457e-440a-aa38-0d585ee512e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Because the requested parallelism was None or a non-positive value, parallelism will be set to (4), which is Spark's default parallelism (4), or 1, whichever is greater. We recommend setting parallelism explicitly to a positive value because the total of Spark task slots is subject to cluster sizing.\nHyperopt with SparkTrials will automatically track trials in MLflow. To view the MLflow experiment associated with the notebook, click the 'Runs' icon in the notebook context bar on the upper right. There, you can view all runs.\nTo view logs from trials, please check the Spark executor logs. To view executor logs, expand 'Spark Jobs' above until you see the (i) icon next to the stage from the trial job. Click it and find the list of tasks. Click the 'stderr' link for a task to view trial logs.\n\r  0%|          | 0/32 [00:00<?, ?trial/s, best loss=?]\r  3%|▎         | 1/32 [00:12<06:17, 12.19s/trial, best loss: -0.0]\r  6%|▋         | 2/32 [00:13<02:50,  5.68s/trial, best loss: -0.0]\r  9%|▉         | 3/32 [00:15<01:58,  4.09s/trial, best loss: -0.5421166306695464]\r 12%|█▎        | 4/32 [00:16<01:20,  2.89s/trial, best loss: -0.5421166306695464]\r 16%|█▌        | 5/32 [00:21<01:38,  3.65s/trial, best loss: -0.5421166306695464]\r 22%|██▏       | 7/32 [00:24<01:04,  2.56s/trial, best loss: -0.5421166306695464]\r 25%|██▌       | 8/32 [00:30<01:23,  3.49s/trial, best loss: -0.5421166306695464]\r 28%|██▊       | 9/32 [00:31<01:05,  2.83s/trial, best loss: -0.5452562704471101]\r 34%|███▍      | 11/32 [00:36<00:56,  2.69s/trial, best loss: -0.5452562704471101]\r 38%|███▊      | 12/32 [00:40<01:00,  3.03s/trial, best loss: -0.545816733067729] \r 41%|████      | 13/32 [00:41<00:47,  2.51s/trial, best loss: -0.545816733067729]\r 44%|████▍     | 14/32 [00:45<00:52,  2.92s/trial, best loss: -0.545816733067729]\r 50%|█████     | 16/32 [00:53<00:51,  3.20s/trial, best loss: -0.586]            \r 53%|█████▎    | 17/32 [00:55<00:43,  2.92s/trial, best loss: -0.586]\r 56%|█████▋    | 18/32 [00:58<00:41,  2.96s/trial, best loss: -0.586]\r 59%|█████▉    | 19/32 [01:04<00:49,  3.79s/trial, best loss: -0.5921450151057401]\r 62%|██████▎   | 20/32 [01:05<00:36,  3.04s/trial, best loss: -0.5921450151057401]\r 66%|██████▌   | 21/32 [01:10<00:39,  3.60s/trial, best loss: -0.5921450151057401]\r 69%|██████▉   | 22/32 [01:12<00:31,  3.15s/trial, best loss: -0.5921450151057401]\r 72%|███████▏  | 23/32 [01:18<00:35,  4.00s/trial, best loss: -0.5921450151057401]\r 78%|███████▊  | 25/32 [01:21<00:20,  2.87s/trial, best loss: -0.5921450151057401]\r 81%|████████▏ | 26/32 [01:28<00:23,  3.93s/trial, best loss: -0.598019801980198] \r 84%|████████▍ | 27/32 [01:30<00:17,  3.44s/trial, best loss: -0.598019801980198]\r 88%|████████▊ | 28/32 [01:32<00:12,  3.06s/trial, best loss: -0.598019801980198]\r 91%|█████████ | 29/32 [01:38<00:11,  3.88s/trial, best loss: -0.598019801980198]\r 94%|█████████▍| 30/32 [01:39<00:06,  3.06s/trial, best loss: -0.598019801980198]\r 97%|█████████▋| 31/32 [01:41<00:02,  2.76s/trial, best loss: -0.598019801980198]\r100%|██████████| 32/32 [01:47<00:00,  3.71s/trial, best loss: -0.598019801980198]\r100%|██████████| 32/32 [01:47<00:00,  3.37s/trial, best loss: -0.598019801980198]\nTotal Trials: 32: 32 succeeded, 0 failed, 0 cancelled.\n"
     ]
    }
   ],
   "source": [
    "algo = tpe.suggest\n",
    "\n",
    "spark_trials = SparkTrials()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    best_result = fmin(\n",
    "        fn = objective, \n",
    "        space = search_space,\n",
    "        algo = algo,\n",
    "        max_evals = 32,\n",
    "        trials = spark_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3eee9ce7-6b92-40fd-af19-e2d7a5021bfa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Using hyperopt.space_eval to display the results of the hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3da93fb5-56ac-4641-a6e3-63ca52527bd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 17, 'n_estimators': 400, 'type': 'rf'}\n"
     ]
    }
   ],
   "source": [
    "import hyperopt\n",
    "\n",
    "print(hyperopt.space_eval(search_space, best_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b09275b-526e-424e-8c5f-597f4f74807c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "HyperparameterTuningUsingHyperoptWithScikitLearn",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
